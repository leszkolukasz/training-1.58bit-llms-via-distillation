\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{Training 1.58bit LLMs via Distillation}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\author{Łukasz Leszko \\
	\texttt{[uzupełnić]} \\
	%% examples of more authors
	\And
	Filip Mateńko \\
	\texttt{f.matenko@student.uw.edu.pl} \\
}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{Training 1.58bit LLMs via Distillation}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Łukasz Leszko},
pdfkeywords={LLM, Distillatio, Quantization},
}

\begin{document}
\maketitle

\begin{abstract}
	\lipsum[1]
\end{abstract}


% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}


\section{Introduction}
In recent years, we have observed rapid growth in Large Language Models (LLMs). They have expanded both in their capabilities and in size. 
Unlike other fields of machine learning, LLMs do not seem to follow the usual rules of overfitting when increasing the number of 
parameters. When properly trained, more parameters generally lead to better performance in this field. 

Unfortunately, the larger these models become, the more sophisticated hardware and computing power they require. Modern LLMs such as 
ChatGPT or DeepSeek-R1 demand multiple industrial-grade GPU accelerators to run efficiently. This requirement excludes individuals and 
organizations without access to such infrastructure from running these models locally, limiting full customization and integration.

Moreover, the energy consumption of human technology is considered one of the major issues of the 21st century. Data centers running LLMs 
consume enormous amounts of energy for both training and inference. One way to address this issue is through quantization—the process of 
reducing the precision of a model’s parameters.\cite{xiao2024smoothquantaccurateefficientposttraining}

Typically, parameters in LLMs are represented in 32-bit precision. The idea is to use lower precisions such as 16-bit, 4-bit, 
or even 1-bit to reduce the memory required to host and run the model. Many researchers around the world are approaching this task from 
different angles. Some claim to achieve performance close to that of unquantized LLMs.\cite{wang2023bitnetscaling1bittransformers} We 
decided to focus on the most extreme forms of quantization—reducing the representation to 1 bit (weights from {-1, 1})—and compare it
with 1.5-bit representation (weights from {-1, 0, 1}). The second approach is less common but has already been introduced in BitNet 
b1.58.\cite{ma2024era1bitllmslarge}

One possible method is to train the LLM in low precision from scratch. However, this approach, like any full training process, is
computationally expensive. Additionally, it poses challenges when computing gradients with respect to discrete-valued parameters.
An alternative is to take an existing high-precision model and distill it into a quantized model.
\cite{du2024bitdistillerunleashingpotentialsub4bit} The full-precision model serves as a teacher to a smaller, quantized student model. 
In our work, we aim to explore an approach similar to that used in FBI-LLM.\cite{fbillm} In this work, the authors first binarize all 
linear transformer weights using a signum function—excluding embeddings, layer norms, and the head. They then introduce additional 
full-precision weights and biases for each binarized linear layer. These parameters, along with the head, become the only learnable 
components after bit quantization. The model is then distilled using a simple cross-entropy loss to align the responses of the student 
model with those of the teacher.

In our approach, we plan to explore both 1-bit and 1.5-bit quantizations. Additionally, we aim to experiment with different loss 
functions, such as KL divergence, Wasserstein distance, and symmetrized KL divergence. These approaches have been investigated in various
prior works.\cite{boizard2025crosstokenizerdistillationuniversallogit, du2024bitdistillerunleashingpotentialsub4bit} Using KL divergence 
could better align the output distributions of the student and teacher models, as opposed to simply learning correct answers, which is 
the focus of cross-entropy loss. Moreover, Wasserstein distance may allow distillation even when the student and teacher have different 
output distributions.

For future work, it would also be valuable to compare training from scratch with distillation-based approaches. Some researchers 
have also explored white-box distillation, which aims to mimic not only the final outputs but also the hidden states of the teacher 
model.\cite{gu2024minillmknowledgedistillationlarge}



\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]



\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, keshet2016prediction} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
