\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{Training 1.58bit LLMs via Distillation}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\author{Łukasz Leszko \\
	\texttt{ll438580@student.mimuw.edu.pl} \\
	%% examples of more authors
	\And
	Filip Mateńko \\
	\texttt{f.matenko@student.uw.edu.pl} \\
}

% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{proposal}
\renewcommand{\shorttitle}{Training 1.58bit LLMs via Distillation}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Łukasz Leszko},
pdfkeywords={LLM, Distillatio, Quantization},
}

\begin{document}
\maketitle

\begin{abstract}
	In this work, we propose evaluation of LLMs distilled from a full-precision model down to 1.58-bit precision. Our evaluation will focus on the impact of different loss functions and quantization methods on the performance of the distilled model. We will try to validate claims that 1.58-bit quantization can achieve performance close to that of unquantized models.
\end{abstract}


% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}


\section{Introduction}
In recent years, we have observed rapid growth in Large Language Models (LLMs). They have expanded both in their capabilities and in size. 
Unlike other fields of machine learning, LLMs do not seem to follow the usual rules of overfitting when increasing the number of 
parameters. When properly trained, more parameters generally lead to better performance in this field. 

Unfortunately, the larger these models become, the more sophisticated hardware and computing power they require. Modern LLMs such as 
ChatGPT or DeepSeek-R1 demand multiple industrial-grade GPU accelerators to run efficiently. This requirement excludes individuals and 
organizations without access to such infrastructure from running these models locally, limiting full customization and integration.

Moreover, the energy consumption of human technology is considered one of the major issues of the 21st century. Data centers running LLMs 
consume enormous amounts of energy for both training and inference. One way to address this issue is through quantization—the process of 
reducing the precision of a model’s parameters \cite{quantizationtechniques}.

Typically, parameters in LLMs are represented in 32-bit precision. The idea is to use lower precisions such as 16-bit, 4-bit, 
or even 1-bit to reduce the memory required to host and run the model. Many researchers around the world are approaching this task from 
different angles. Some claim to achieve performance close to that of unquantized LLMs \cite{wang2023bitnetscaling1bittransformers}. We 
decided to focus on the most extreme forms of quantization—reducing the representation to 1 bit (weights from {-1, 1})—and compare it
with 1.5-bit representation (weights from {-1, 0, 1}). The second approach is less common but has already been introduced in BitNet 
b1.58 \cite{ma2024era1bitllmslarge}.

One possible method is to train the LLM in low precision from scratch. However, this approach, like any full training process, is
computationally expensive. Additionally, it poses challenges when computing gradients with respect to discrete-valued parameters.
An alternative is to take an existing high-precision model and distill it into a quantized model
\cite{du2024bitdistillerunleashingpotentialsub4bit}. The full-precision model serves as a teacher to a smaller, quantized student model. 
In our work, we aim to explore an approach similar to that used in FBI-LLM \cite{fbillm}. In this work, the authors first binarize all 
linear transformer weights using a signum function—excluding embeddings, layer norms, and the head. They then introduce additional 
full-precision weights and biases for each binarized linear layer. These parameters, along with the head, become the only learnable 
components after bit quantization. The model is then distilled using a simple cross-entropy loss to align the responses of the student 
model with those of the teacher.

In our approach, we plan to explore both 1-bit and 1.58-bit quantizations. Additionally, we aim to experiment with different loss 
functions, such as KL divergence, Confidence-Aware KL divergence, Wasserstein distance. These approaches have been investigated in various
prior works \cite{boizard2025crosstokenizerdistillationuniversallogit, du2024bitdistillerunleashingpotentialsub4bit}. Using KL divergence 
could better align the output distributions of the student and teacher models, as opposed to simply learning correct answers, which is 
the focus of cross-entropy loss. Moreover, Wasserstein distance may allow distillation even when the student and teacher have different 
output distributions.

For future work, it would also be valuable to compare training from scratch with distillation-based approaches. Some researchers 
have also explored white-box distillation, which aims to mimic not only the final outputs but also the hidden states of the teacher 
model \cite{gu2024minillmknowledgedistillationlarge}.

\section{Experimental setup}

Choosing model size is crucial for our experiments. Results from \cite{ma2024era1bitllmslarge} show that starting from 3B parameters, a 1.58 distilled student was able to much or even outperform it's teacher model. Taking that into account, as well as the fact that we are limited by the computing power, we decided to use a model in a similar size range (2-3B parameters). The choice of the actual architecture will be made later, based on empirical results.

 We follow the quantization process proposed in BitNet and BitNet b1.58 which replaces linear layers with a custom linear layer (BiLinear) that performs 1-bit or 1.58-bit quantization on weights during training. This approach allows us to easily add 1-bit and 1.58-bit quantization capabilities to any model with linear layers (not only transformers).

 To perform the distillation, we extend the FBI-LLM approach to support 1.58-bit quantization simply by replacing the quantization function with the one proposed in BitNet b1.58. While FBI-LLM uses cross-entropy loss, we will also experiment with the following loss functions:

\begin{itemize}
	\item (Forward) KL Divergence - a measure of how teacher's output distribution differs from the student's output distribution
	\item Confidence-Aware KL divergence - weighted sum of forward and backward KL divergence with weights based on the confidence of teacher model's predictions
	\item Wasserstein Distance - metric for quantifying dissimilarities between distributions stemming from the optimal transport theory
\end{itemize}

The architecture of the BiLinear layer proposed in FBI-LLM closely follows the one used in OneBit \cite{onebit} while differes from the one used in BitNet b1.58 by adding small full-precision learnable parameters. We will investigate how different implementations of the BiLinear layer affect the performance of the distilled model. To that end, we will compare the BiLinear implementations from FBI-LLM, OneBit, and BitNet b1.58.

The baseline for our experiments will be 1-bit LLM, which has been trained using the exact FBI-LLM approach using the Amber dataset. Amber dataset is an agglomeration of Refined-Web, StarCoder, and RedPajama-v1 and contains the total 1.26 trillion tokens. We will also utilise parts of this dataset to train our 1.58-bit LLMs.

To evaluate our models, we will use EleutherAI's evaluation suite which provides a set of benchmarks for LLMs. We will focus on measuring perplexity and accuracy on various datasets, like WikiText, MMLU, and others. In addition, as was noted in FBI-LLM paper, training extremely quantized models with distillation may be unstable and result in models that fail to converge. We will monitor that stability by tracking the flip flop rate, which is the percentage of quantized weights that changed sign during training.

One thing to note is that while the quantized weights, on a disk, can be stored in a packed format, during training, we still need to store them in float16 format to compute gradients. Similarly, during inference, multiplying by 1.58 quantized matrix does not require \verb|MUL| operator, however, to benefit from that, we need to utilise a custom hardware and kernels which is outside of our expertise.
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
